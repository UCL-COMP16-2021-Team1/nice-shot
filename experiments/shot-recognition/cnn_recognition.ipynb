{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-based 3D Skeleton Tennis Shot Recognition\n",
    "\n",
    "Based on method from paper ['Skeleton-based Action Recognition with Convolutional Neural Networks'](https://arxiv.org/abs/1704.07595) by Li et al. (2017).\n",
    "\n",
    "We use the 3D skeletal animation data from the ['THETIS'](http://thetis.image.ece.ntua.gr/) dataset for training. Each animation is represented as a `T x N` 3-channel image where `T` is the number of frames of the animation, `N` is the number of skeletal joints, and the 3 colour channels represent the `x, y, z` coordinates of the joints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the skeleton images training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1974 files belonging to 4 classes.\n",
      "Using 1580 files for training.\n",
      "Found 1974 files belonging to 4 classes.\n",
      "Using 394 files for validation.\n",
      "['backhand', 'forehand', 'service', 'smash']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "skeleton_dir = \"data/skeleton_images\"\n",
    "\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  skeleton_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  skeleton_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./127.5, offset=-1, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 2s 27ms/step - loss: 0.9145 - accuracy: 0.6595 - val_loss: 0.6230 - val_accuracy: 0.7665\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5052 - accuracy: 0.8057 - val_loss: 0.4474 - val_accuracy: 0.8198\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4094 - accuracy: 0.8367 - val_loss: 0.4192 - val_accuracy: 0.8350\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3320 - accuracy: 0.8608 - val_loss: 0.3245 - val_accuracy: 0.8807\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.2717 - accuracy: 0.8899 - val_loss: 0.2849 - val_accuracy: 0.8985\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.2173 - accuracy: 0.9127 - val_loss: 0.2391 - val_accuracy: 0.9061\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.2089 - accuracy: 0.9146 - val_loss: 0.2562 - val_accuracy: 0.9036\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1684 - accuracy: 0.9342 - val_loss: 0.2364 - val_accuracy: 0.9086\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1477 - accuracy: 0.9342 - val_loss: 0.2376 - val_accuracy: 0.9213\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1574 - accuracy: 0.9386 - val_loss: 0.2065 - val_accuracy: 0.9137\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=10\n",
    ")\n",
    "\n",
    "model.save('cnn_recognition_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c214f4d2c295345fdc57900708451324837b7865065ae46ceb283a6c9824d5d4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
